{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Check GPU allocation\n",
        "# We want to see a table with GPU details (e.g., Tesla T4)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZJmecE9iXiU",
        "outputId": "303e8831-3ad9-429c-c8df-f2e223f212f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 11 10:38:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Ultralytics library which includes YOLOv10 support.\n",
        "!pip install -q ultralytics\n",
        "\n",
        "# Import necessary libraries\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Check the installed version\n",
        "print(ultralytics.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVrWSC3tih_U",
        "outputId": "55fd337e-cba7-4986-c6d3-5462ee3ae9a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "8.3.153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install the necessary library\n",
        "!pip install -q ultralytics\n",
        "\n",
        "# 2. Import the YOLO class\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# 3. Load the YOLOv10n model\n",
        "# The '.pt' file will be downloaded automatically if it's not found.\n",
        "model = YOLO(\"yolov10s.pt\")\n",
        "\n",
        "# 4. Train the model on the SKU-110K dataset\n",
        "# The dataset will be downloaded automatically on the first run.\n",
        "# This is a large dataset (11 GB), so the initial download may take time.\n",
        "results = model.train(data=\"SKU-110K.yaml\", epochs=10, imgsz=640, batch=8)\n",
        "\n",
        "print(\"\\nTraining complete. Results saved in the 'runs' directory.\")"
      ],
      "metadata": {
        "id": "HaJtcseXik34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SETUP FOR THE FEATURE EXTRACTOR MODEL ===\n",
        "\n",
        "# 1. Mount your Google Drive\n",
        "# This allows us to save models and datasets permanently.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# 2. Install new libraries for Metric Learning\n",
        "# These are different from the YOLO libraries.\n",
        "print(\"\\nInstalling required libraries: PyTorch Metric Learning and Timm...\")\n",
        "!pip install -q pytorch-metric-learning\n",
        "!pip install -q timm\n",
        "print(\"Installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFiilJJBkBrf",
        "outputId": "6d9a0e2b-a740-44be-a9f4-d1dd6e723f8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Installing required libraries: PyTorch Metric Learning and Timm...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstallation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics torchvision faiss-cpu opencv-python matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoiNd9EZNGAk",
        "outputId": "9ff0f20b-1cc6-4e05-9654-ebf451f75aaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import faiss\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Icn3pOjSga0",
        "outputId": "af386a77-f2a4-4896-ea42-5d2572ae526f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet = models.resnet34(pretrained=True)\n",
        "resnet.fc = torch.nn.Identity()\n",
        "resnet = resnet.to(device).eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK0ebc4PS1UM",
        "outputId": "ef10a467-6365-460d-89ee-17ddacb5ea74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 116MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def spatial_weight(d, sigma=0.3):\n",
        "    return np.exp(-d**2 / (2 * sigma**2))\n",
        "\n",
        "def apply_caqe(vectors, positions):\n",
        "    refined = []\n",
        "    for i, q in enumerate(vectors):\n",
        "        q_prime = q.copy()\n",
        "        for j, n in enumerate(vectors):\n",
        "            if i == j: continue\n",
        "            alpha = cosine_similarity(q, n)\n",
        "            dx = positions[i][0] - positions[j][0]\n",
        "            dy = positions[i][1] - positions[j][1]\n",
        "            dist = np.sqrt(dx**2 + dy**2)\n",
        "            beta = spatial_weight(dist)\n",
        "            q_prime += alpha * beta * n\n",
        "        q_prime = q_prime / np.linalg.norm(q_prime)\n",
        "        refined.append(q_prime)\n",
        "    return refined"
      ],
      "metadata": {
        "id": "cJXW2ZwVS3v0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dir = \"/content/drive/My Drive/clean_sku_dataset\"\n",
        "sku_vectors = []\n",
        "sku_labels = []\n",
        "\n",
        "for sku_id in os.listdir(reference_dir):\n",
        "    sku_path = os.path.join(reference_dir, sku_id)\n",
        "    if not os.path.isdir(sku_path): continue\n",
        "\n",
        "    raw_vectors = []\n",
        "    positions = []\n",
        "\n",
        "    for i, img_file in enumerate(os.listdir(sku_path)):\n",
        "        img_path = os.path.join(sku_path, img_file)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vec = resnet(tensor).squeeze().cpu().numpy()\n",
        "            vec = vec / np.linalg.norm(vec)\n",
        "\n",
        "        raw_vectors.append(vec)\n",
        "        positions.append((i, 0))  # fake x position (same SKU = nearby)\n",
        "\n",
        "    # Apply CAQE to SKU group\n",
        "    refined_vectors = apply_caqe(raw_vectors, positions)\n",
        "\n",
        "    for vec in refined_vectors:\n",
        "        sku_vectors.append(vec)\n",
        "        sku_labels.append(sku_id)\n",
        "\n",
        "print(f\"âœ… Stored {len(sku_vectors)} CAQE-refined vectors for {len(set(sku_labels))} SKUs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCcoYpH7S6xM",
        "outputId": "a43c3c34-f359-4496-dbc2-45708532af17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Stored 40 CAQE-refined vectors for 4 SKUs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = sku_vectors[0].shape[0]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(np.stack(sku_vectors))\n",
        "print(\"âœ… FAISS index created with\", index.ntotal, \"vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuU9wz6TS_ro",
        "outputId": "9ef7ea48-ed13-4885-86fc-9d38260cfc5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… FAISS index created with 40 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "shelf_img_path = list(uploaded.keys())[0]\n",
        "print(f\"ğŸ“· Shelf image uploaded: {shelf_img_path}\")"
      ],
      "metadata": {
        "id": "YP1D8ACRcbGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained YOLOv10s\n",
        "yolo_model = YOLO(\"runs/detect/yolov10s/weights/best.pt\")\n",
        "\n",
        "results = yolo_model.predict(source=shelf_img_path, save=False, conf=0.4)\n",
        "pred = results[0]\n",
        "boxes = pred.boxes.xyxy.cpu().numpy()\n",
        "img = cv2.imread(shelf_img_path)\n",
        "print(f\"âœ… Detected {len(boxes)} products.\")"
      ],
      "metadata": {
        "id": "3cnx26A-chR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"shelf_crops\", exist_ok=True)\n",
        "crop_paths = []\n",
        "\n",
        "for i, box in enumerate(boxes):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    crop = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "    crop_path = f\"shelf_crops/crop_{i}.jpg\"\n",
        "    cv2.imwrite(crop_path, crop)\n",
        "    crop_paths.append(crop_path)"
      ],
      "metadata": {
        "id": "8PTWb8SNciR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vecs = []\n",
        "positions = []\n",
        "\n",
        "for i, path in enumerate(crop_paths):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vec = resnet(tensor).squeeze().cpu().numpy()\n",
        "        vec = vec / np.linalg.norm(vec)\n",
        "\n",
        "    query_vecs.append(vec)\n",
        "\n",
        "    # use center of box for CAQE\n",
        "    x1, y1, x2, y2 = boxes[i]\n",
        "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "    positions.append((cx, cy))"
      ],
      "metadata": {
        "id": "AH0CV57eckHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refined_query_vecs = apply_caqe(query_vecs, positions)"
      ],
      "metadata": {
        "id": "X4_8yDxjcmMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D, I = index.search(np.stack(refined_query_vecs), k=1)\n",
        "matched_skus = [sku_labels[i[0]] for i in I]\n",
        "\n",
        "print(\"\\nğŸ¯ Detected SKUs:\")\n",
        "for i, sku in enumerate(matched_skus):\n",
        "    print(f\"  â€¢ Product {i+1}: {sku}\")"
      ],
      "metadata": {
        "id": "l3xUsdDycnxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sku_list = sorted(set(sku_labels))"
      ],
      "metadata": {
        "id": "T-o8qQprUAr8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sku_list = sorted(set(sku_labels))\n",
        "print(\"\\nğŸ›ï¸ Available SKUs:\")\n",
        "for i, sku in enumerate(sku_list):\n",
        "    print(f\"{i}: {sku}\")\n",
        "\n",
        "selected = input(\"ğŸ” Enter brand name to focus on (e.g., 'pepsi'): \").strip().lower()\n",
        "matched_targets = [sku for sku in sku_list if selected in sku.lower()]\n",
        "\n",
        "if not matched_targets:\n",
        "    print(\"âŒ No matching SKU found.\")\n",
        "else:\n",
        "    print(f\"âœ… Matched Brand(s): {matched_targets}\")"
      ],
      "metadata": {
        "id": "rVB1YcHFcN0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_boxes = []\n",
        "selected_labels = []\n",
        "selected_centers = []\n",
        "\n",
        "for i, (sku, box) in enumerate(zip(matched_skus, boxes)):\n",
        "    if sku in matched_targets:\n",
        "        selected_boxes.append(box)\n",
        "        selected_labels.append(sku)\n",
        "        x1, y1, x2, y2 = box\n",
        "        cx = (x1 + x2) / 2\n",
        "        selected_centers.append(cx)"
      ],
      "metadata": {
        "id": "LaOq85yccP89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "sku_counts_filtered = Counter(selected_labels)\n",
        "\n",
        "print(\"\\nğŸ§® Product Counts for Selected Brand:\")\n",
        "for sku, count in sku_counts_filtered.items():\n",
        "    print(f\"{sku}: {count}\")"
      ],
      "metadata": {
        "id": "vMugkV_6cxOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_cells = 10  # Expected positions across shelf\n",
        "img_h, img_w = img.shape[:2]\n",
        "cell_width = img_w / grid_cells\n",
        "\n",
        "detected_cells = set()\n",
        "\n",
        "for cx in selected_centers:\n",
        "    cell_index = int(cx / cell_width)\n",
        "    detected_cells.add(cell_index)\n",
        "\n",
        "missing_cells = set(range(grid_cells)) - detected_cells\n",
        "print(f\"\\nğŸ•³ï¸ Shelf Gaps for selected brand: {sorted(missing_cells)}\")"
      ],
      "metadata": {
        "id": "Z6RooOgwcy9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_img = img.copy()\n",
        "\n",
        "# Draw selected brand boxes\n",
        "for i, box in enumerate(selected_boxes):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    cv2.rectangle(annotated_img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "    cv2.putText(annotated_img, selected_labels[i], (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "\n",
        "# Draw missing slots\n",
        "for gap in missing_cells:\n",
        "    cx = int(gap * cell_width + cell_width / 2)\n",
        "    cy = int(img_h * 0.95)\n",
        "    cv2.circle(annotated_img, (cx, cy), radius=10, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "# Show result\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(False)\n",
        "plt.title(f\"ğŸ§ƒ Annotated Detection for: {matched_targets}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4wtgNBdc0vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pC3t6lIBc2cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}